#!/usr/bin/env python3
"""
AI äº§å“æ›´æ–°æ—¥å¿—è‡ªåŠ¨æ”¶é›†ç³»ç»Ÿï¼ˆç´¯ç§¯æ¨¡å¼ï¼‰
- ä¸€ä¸ªäº§å“ä¸€ä¸ªå›ºå®šæ–‡ä»¶
- æŒ‰ç‰ˆæœ¬å·å»é‡å’Œè¦†ç›–
- æ—¶é—´é€†åºæ’åˆ—
"""

import json
import os
import time
from pathlib import Path
from datetime import datetime
from scrapers import create_scraper
from scrapers.parser import ChangelogParser
from platform_compat import setup_stdio_encoding

# è·¨å¹³å°å…¼å®¹æ€§è®¾ç½®
setup_stdio_encoding()


# é…ç½®è·¯å¾„
CONFIG_FILE = Path(__file__).parent / 'config' / 'products.json'
DATA_DIR = Path(__file__).parent / 'data' / 'raw'


def load_config():
    """åŠ è½½äº§å“é…ç½®"""
    with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
        return json.load(f)


def read_existing_versions(product_name: str, content: str) -> set:
    """
    ä»ç°æœ‰æ–‡ä»¶ä¸­æå–å·²å­˜åœ¨çš„ç‰ˆæœ¬å·

    Returns:
        ç‰ˆæœ¬å·é›†åˆï¼Œå¦‚ {"2.1.37", "2.1.36", ...}
    """
    parser = ChangelogParser(days=30)

    # ä½¿ç”¨é€šç”¨è§£æå™¨æå–ç‰ˆæœ¬å·
    versions = set()
    pattern = r'^## \[([^\]]+)\]'

    import re
    for line in content.split('\n'):
        match = re.match(pattern, line)
        if match:
            versions.add(match.group(1))

    return versions


def merge_and_save_updates(product_name: str, existing_versions: set, new_content: str):
    """
    åˆå¹¶æ–°æ—§æ•°æ®ï¼ŒæŒ‰æ—¶é—´é€†åºä¿å­˜

    Args:
        product_name: äº§å“åç§°
        existing_versions: å·²å­˜åœ¨çš„ç‰ˆæœ¬å·é›†åˆ
        new_content: æ–°çˆ¬å–çš„å®Œæ•´å†…å®¹
    """
    # è§£ææ–°æ•°æ®ä¸­çš„ç‰ˆæœ¬
    parser = ChangelogParser(days=30)
    new_updates = parser.parse(product_name, new_content)

    # è¯»å–æ—§æ•°æ®
    data_file = DATA_DIR / f"{product_name}.md"
    old_updates = []

    if data_file.exists():
        old_content = data_file.read_text(encoding='utf-8')
        old_updates = parser.parse(product_name, old_content)

    # æ„å»ºç‰ˆæœ¬åˆ°å†…å®¹çš„æ˜ å°„ï¼ˆæ—§æ•°æ®ï¼‰
    version_map = {}
    for update in old_updates:
        version = update['version']
        version_map[version] = update

    # ç”¨æ–°æ•°æ®æ›´æ–°/æ·»åŠ ç‰ˆæœ¬
    for update in new_updates:
        version = update['version']
        version_map[version] = update  # æ–°ç‰ˆæœ¬è¦†ç›–ï¼Œå·²å­˜åœ¨ç‰ˆæœ¬ä¹Ÿç”¨æ–°æ•°æ®è¦†ç›–

    # æŒ‰æ—¥æœŸæ’åºï¼ˆæ—¶é—´é€†åºï¼‰
    all_updates = list(version_map.values())
    all_updates.sort(key=lambda x: x['date'] or datetime.min, reverse=True)

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    DATA_DIR.mkdir(parents=True, exist_ok=True)

    # å†™å…¥æ–‡ä»¶
    lines = []
    lines.append(f"# {product_name} Changelog\n")
    lines.append(f"æœ€åæ›´æ–°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    lines.append("")

    for update in all_updates:
        version = update['version']
        date = update['date']
        content = update['content']

        if date:
            date_str = date.strftime('%Y-%m-%d')
            lines.append(f"## [{version}] - {date_str}")
        else:
            lines.append(f"## [{version}]")

        lines.append("")
        lines.append(content)
        lines.append("")
        lines.append("")

    output = '\n'.join(lines)

    with open(data_file, 'w', encoding='utf-8') as f:
        f.write(output)

    print(f"  âœ“ å·²æ›´æ–°æ–‡ä»¶: {data_file}")
    print(f"    æ€»ç‰ˆæœ¬æ•°: {len(all_updates)}")
    print(f"    æ–°å¢ç‰ˆæœ¬: {len(new_updates)}")

    return data_file


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("AI äº§å“æ›´æ–°æ—¥å¿—æ”¶é›†ç³»ç»Ÿï¼ˆç´¯ç§¯æ¨¡å¼ï¼‰")
    print("=" * 60)
    print()

    # åŠ è½½é…ç½®
    config = load_config()
    products = config['products']

    print(f"æ‰¾åˆ° {len(products)} ä¸ªäº§å“éœ€è¦è¿½è¸ª")
    print()

    # æ”¶é›†æ‰€æœ‰äº§å“çš„æ›´æ–°æ—¥å¿—
    results = []

    for product in products:
        try:
            product_name = product['name']

            # è¯»å–ç°æœ‰æ–‡ä»¶
            data_file = DATA_DIR / f"{product_name}.md"
            existing_versions = set()

            if data_file.exists():
                print(f"ğŸ“‚ {product_name}: å‘ç°ç°æœ‰æ–‡ä»¶")
                existing_content = data_file.read_text(encoding='utf-8')
                existing_versions = read_existing_versions(product_name, existing_content)
                print(f"   å·²æœ‰ {len(existing_versions)} ä¸ªç‰ˆæœ¬")
            else:
                print(f"ğŸ†• {product_name}: é¦–æ¬¡çˆ¬å–")

            # åˆ›å»ºçˆ¬è™«å¹¶è·å–æ–°æ•°æ®
            scraper = create_scraper(product)
            new_content = scraper.fetch()

            # åˆå¹¶å¹¶ä¿å­˜
            filepath = merge_and_save_updates(product_name, existing_versions, new_content)

            results.append({
                'name': product_name,
                'url': product.get('url', ''),
                'status': 'success',
                'file': str(filepath)
            })

            print()

            # äº§å“ä¹‹é—´æ·»åŠ 0.5ç§’å»¶è¿Ÿï¼Œé¿å…è§¦å‘APIé™æµ
            time.sleep(0.5)

        except Exception as e:
            print(f"  âœ— å¤±è´¥: {e}")
            print()
            results.append({
                'name': product['name'],
                'url': product.get('url', ''),
                'status': 'failed',
                'error': str(e)
            })

    # æ±‡æ€»æŠ¥å‘Š
    print("=" * 60)
    print("çˆ¬å–å®Œæˆï¼")
    print("=" * 60)
    print()

    success_count = sum(1 for r in results if r['status'] == 'success')
    print(f"æˆåŠŸ: {success_count}/{len(results)}")

    for r in results:
        if r['status'] == 'success':
            print(f"  âœ… {r['name']}: {r['file']}")
        else:
            print(f"  âŒ {r['name']}: {r.get('error', 'Unknown error')}")


if __name__ == '__main__':
    main()
